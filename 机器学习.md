





## 导学

### 样本  

也称为“**示例**”，是关于一个事件或对象的描述。



#### 向量--特征--特征值

显然，线性代数中的向量就很适合，因为任何事物都可以由若干“特征”（或称为“**属性**”）唯一刻画出来，而**向量**的各个维度即可用来描述各个**特征**。

（向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量）相应特征的取值，也称为“属性值”。

### 样本空间

也称为“输入空间”或“属性空间”。由于样本采用的是标明各个特征取值的“特征向量” 来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在的空间为样本空间，通常用花式大写的 X 表示。



### 数据集

数据集通常用集合来表示，令集合 D = {x1, x2, ..., xm} 表示包含 m 个样本的数据集，一般 同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有 d 个特征，则第 i 个样本的数学表示为 d 维向量：xi = (xi1; xi2; ...; xid)，其中 xij 表示样本 xi 在第 j 个属性上的取值。

### 模型

机器学习的一般流程如下：首先收集若干样本（假设此时有 100 个），然后将其分为训练样本 （80 个）和测试样本（20 个），其中 80 个训练样本构成的集合称为“训练集”，20 个测试样本构成的集合 称为“测试集”，接着选用某个机器学习算法，让其在训练集上进行“学习”（或称为“训练”），然后产出 得到“模型”（或称为“学习器”），最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默认 样本的背后是存在某种潜在的规律，我们称这种潜在的规律为“真相”或者“真实”，例如样本是一堆好西 瓜和坏西瓜时，我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开。当我们应用某个机 器学习算法来学习时，产出得到的模型便是该算法所找到的它自己认为的规律，由于该规律通常并不一定就是所谓的真相，所以也将其称为“假设”。通常机器学习算法都有可配置的参数，同一个机器学习算法， 使用不同的参数配置或者不同的训练集，训练得到的模型通常都不同。

从数据中学得模型的过程称为“**学习**”（learning）或 “**训练**”（training）, 这个过程通过执行某个学习算法来完成.训练过程中使用的数据称为“**训练 数 据** "（training data）, 其中每个样本称为一个“**训练样本**" （training sample）, 训练样本组成的集合称为“**训 练 集** "（training set）. 学得模型对应了关于数据 的某种潜在的规律，因此亦称“**假 设** "（hypothesis）; 这种潜在规律自身,则称 为 “**真相**”或 “真实”（ground-truth）, 学习过程就是为了找出或逼近真相.本 书有时将模型称为“**学习器**”（learner）

### 标记

上文提到机器学习的本质就是在学习**样本**在**某个方面**的表现是否存在**潜在的规律**，我们称**该方面的信息**为“标记”   例如在学习西瓜的好坏时，“好瓜”和“坏瓜”便是样本的标记。一般第 i 个样本的 标记的数学表示为 yi，**标记**所在的空间称为“**标记空间**”或“**输出空间**”，数学表示为花式大写的 Y。**标记**通常也看作为样本的一部分，因此，一个完整的样本通常表示为 **(x, y)**。



#### 分类

当标记取值为**离散型**时，称此类任务为“**分类**”，例如学习西瓜是好瓜还是坏瓜、学习猫的图片是白猫还是黑猫等。当分类的类别只有两个时，称此类任务为“**二分类**”，通常称其中一个为“**正类**”，另 一个为“**反类**”或“**负类**”；当分类的类别超过两个时，称此类任务为“**多分类**”。由于标记也属于样本的一部分，通常也需要参与运算，因此也需要将其数值化，例如对于二分类任务，通常将正类记为 1，反类记为 0，即 Y = {0, 1}。这只是一般默认的做法，具体标记该如何数值化可根据具体机器学 习算法进行相应地调整，例如第 6 章的支持向量机算法则采用的是 Y = {−1, +1}；

 • 当标记取值为**连续型**时，称此类任务为“**回归**”，例如学习预测西瓜的成熟度、学习预测未来的房价 等。由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，**回归任务的标记取值范围通常是整个实数域 R**，即 **Y = R。**

无论是分类还是回归，机器学习算法最终学得的模型都可以抽象地看作为以样本 x 为自变量，标记 y 为因变量的函数 y = f(x)，即一个从输入空间 X 到输出空间 Y 的映射。例如在学习西瓜的好坏时，机器 学习算法学得的模型可看作为一个函数 f(x)，给定任意一个西瓜样本 xi = (青绿; 蜷缩; 清脆)，将其输入 进函数即可计算得到一个输出 yi = f(xi)，此时得到的 yi 便是模型给出的预测结果，当 yi 取值为 1 时表 明模型认为西瓜 xi 是好瓜，当 yi 取值为 0 时表明模型认为西瓜 xi 是坏瓜。

### 测试

学得模型后，使用其进行预测的过程称为“测试”(testing),被预测的样本 称 为 “**测试样本**”(testing sample)

### clustering

我们还可以对西瓜做“**聚类** " (clustering),即将训练集中的西瓜分成若干组，每组称为一个“簇”(cluster);这些自动形成的簇可能对应一些潜在的概念划分，例如 “浅色瓜” “深 色 瓜 "甚 至 “本地瓜” “外地瓜”.这样的学习过程有助于我们了解数据内在的规律，能为更深入地分析数据建立基础.需说明的是，在聚类学习中，“浅色瓜” “本地瓜”这样的概念我们事先是不知道的, 而且学习过程中使用的**训练样本**通常不拥有标记信息.



根据训练数据是否拥有**标记信息**，学习任务可大致划分为两大类： “**监督 学习**“(supervised learning)和 “**无监督学习** "(unsupervised learning),分类 和回归是前者的代表,而聚类则是后者的代表.



### 监督学习

### 无监督学习









浅层学习







表示学习--特征学习





### 泛 化

学得模型适用于 新样本的能力，称 为 “**泛化** " (generalization)能力.具有强泛化能力的模型能 很好地适用于整个样本空间

通常假设样本空间中全 体样本服从一个未知“分 布 " (distribution ) d，我们获得的每个样本都是独立 地从这个分布上采样获得的，即 “独立同分布" (independent and identically distributed,）简 称 iid⑷ .

### 假设空间

**归纳**(induction)与**演绎**(deduction)是科学推理的两大基本手段.前者是从特殊到一般的“泛化”(generalization)过程，即从具体的事实归结出一般性规律；后者则是从一般到特殊的“特化”(specialization)过程，即从基础原理推演出具体状况.例如，在数学公理系统中，基于一组公理和推理规则推导出与之相洽的定理，这是演绎；而 “从样例中学习”显然是一个**归纳**的过程，因此亦称 “**归纳学习** ”(inductive learning).

归纳学习有**狭义**与**广义**之分，广义的归纳学习大体相当于从样例中学习, 而狭义的归纳学习则要求从训练数据中学得**概念(concept)**,因此亦称为“概念 学 习 ”或 “**概念形成**”.概念学习技术目前研究、应用都比较少，因为要学得 泛化性能好且语义明确的概念实在太困难了，现实常用的技术大多是产生“黑 箱”模型.





### 归纳偏好

若仅有表1 .1中的训练样本，则无法断定上述三个假设中哪一个“更好”. 然而，对于一个具体的学习算法而言，它必须要产生一个模型.这时，学习算 法本身的“偏好”就会起到关键的作用.例如，若我们的算法喜欢“尽可能特 殊”的模型,则它会选择“好瓜分 （色泽= *）A （根蒂= 蜷缩）八（敲声= 浊响）”； 但若我们的算法喜欢“尽可能一般”的模型，并且由于某种原因它更“相信” 根蒂,则它会选择“好瓜O （色泽= *）A （根蒂= 蜷缩）八（敲声= *）”. 机器学习 算法在学习过程中对某种类型假设的偏好，称 为 “**归纳偏好”（inductive bias）, 或简称为“偏好”.**

归纳偏好的作用在图1.3这个回归学习图示中可能更直观.这里的每个训 练样本是图中的一个点（对沙）,要学得一个与训练集一致的模型，相当于找到一 条穿过所有训练样本点的曲线.显然，对有限个样本点组成的训练集，存在着 很多条曲线与其一致.我们的学习算法必须有某种偏好，才能产出它认为“正 确”的模型.例如，若认为相似的样本应有相似的输出（例如，在各种属性上都很相像的西瓜，成熟程度应该比较接近），则对应的学习算法可能偏好图L 3 中 比 较 “**平滑**”的曲线A 而不是比较“**崎岖**”的曲线B.

![image-20240430194138592](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240430194138592.png)



### 误差

![image-20240430194454042](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240430194454042.png)







## 基础

FP32的定义

- 默认情况下，浮点数在计算中会被视为 1.xxxx 形式，因此最高有效位默认为 **1**（隐含的位）。

$$
V=(−1)^符号位×2 ^(指数部分-127)×(1+尾数)
$$

- **指数部分**：所有位为 **0**（这是非规格化数的标志）。非规格数

- **尾数部分（23位）**：不包含隐含的 1，直接表示小数部分。



### 高斯噪声

这种噪声符合“高斯分布”（也称正态分布）

### 正则化

- 一般对一个X*X^T进行正则化



### 汉明距离

- 不同位置上不相同的数的个数

### 欧式距离

- 勾股定理





### 不等式

- 平方和  ----和平方------积





















## 线性代数

- 行秩等于列秩等于秩



### 化最简型

- 从上到下 **最终化**一行
- 一行确定后对列处理





### 矩阵的n阶导数

$$
f(β)=f(β_0)+Σ^∞_{k=1}\frac1{k!}▽^kf(β_0)[(β-β_0)...(β-β_0)]
$$





### 特征值

$$
Av=b
$$

$$
Av=λv
$$

$$
det(A−λI)=0
$$

- 求解以上内容
- v为特征向量  λ为特征值

### 秩

- 满秩矩阵与缺秩矩阵 m*n 矩阵，考虑到m>n则一定为缺秩矩阵







### SVD变换

$$
\sqrt[2]{λ_{i}}=δ_i
$$



- 奇异值分解
  $$
  A = UΣV^T
  $$

  - m*n   ---m*m ---m*n -----n*n



### 向量空间

#### 解空间

- 基向量张成的空间

#### 零空间

$$
N(A)=x∈R 
^n∣Ax=0
$$

- （列）零空间即指在m*n矩阵中，总rank=n的时候在rank=r的方向给他压缩到0，实现降维
- 零空间可以通过如下例子求解
- t,s为自由变量

$$
x_1
​
 +2x_2
​
 +3x_3
​
 =0
$$


$$
x=t\left[\begin{array}{c}
-2 \\
1 \\
0
\end{array}\right]+s\left[\begin{array}{c}
-3 \\
0 \\
1
\end{array}\right], \quad t, s \in \mathbb{R}
$$


### 降维

- 左乘变量









### 范数











### 正规定义

$$
AA^H=A^HA=Q
$$

- 一个矩阵是正规矩阵当且仅当它可以被U对角化

#### 正交矩阵

- ai为列向量（行向量）
  $$
  AA^H=A^HA=I
  $$
  

$$
a_{i}^{T} a_{j}=\left\{\begin{array}{ll}
1, & i=j \\
0, & i \neq j
\end{array}\right.
$$



#### 酉矩阵

$$
U^HU=I
$$





















**找到第一个非零列**  			---先确定行，处理该行

- 从第一列开始，找到矩阵中从上到下第一个非零元素所在的行，称为**主元行**。
- 如果该列中所有元素均为 0，则跳到下一列。

**交换行**

- 如果找到的第一个非零元素不在当前处理的行（例如，第一个非零元素在下面的行），交换两行，使非零元素位于当前处理的行。

**将主元归一化**

- 将主元所在的行通过乘法操作，将**主元变为 1**。即，用该行中的主元除以主元自身，使得主元变为 1。

**消除其他行中主元所在列的元素**   			--按列处理

- 使用主元所在的行，将该列中其他行的对应元素变为**0**。
- 具体操作是：用当前行乘以一个合适的常数，并**加/减**到其他行，使得主元所在列中的其他元素都变为 0。















## 模型评估与选择

通常我们把分类错误的样本数占样本总数的比例称为“**错误率**”(error rate)

即如果在m 个样本中有a 个样本分类错误，则错误率E = a /m ;相应的, 1 —a /m 称 为 “**精度**”(accuracy)

### 误差

我们把 学习器的实际预测输出与样本的真实输出之间的差异称为“**误差**”(error), 学 习 器 在 训 练 集 上 的 误 差 称 为 “**训练误差** ”(training error)或 “**经验误 差**”(empirical e rro r),在 新 样 本 上 的 误 差 称 为 “**泛化误差**”(generalization error)

### 过拟合与欠拟合

很可能已经把训练样本自身的一些**特点**当作了所有**潜在样本**都 会具有的**一般性质**，这样就会导致泛化性能下降.这种现象在机器学习中称为 “过拟合" (overfitting).与 “过拟合”相对的是“欠拟合" (imderRtting),这 是指对训练样本的一般性质尚未学好

 需使用一个“测试集”(testing set)来测试学习器对新样本的判别能
力，然后以测试集上的“测试误差”(testing error)作为泛化误差的近似



#### 留出法

“**留出法**”(hold-out)直接将数据集。划分为两个互斥的集合，其中一个 集合作为训练集S ,另一个作为测试集T , 即 0 = S U T, S n T = 0 .在 S 上训 练出模型后，用 T 来评估其测试误差，作为对泛化误差的估计.

需注意的是，训练/测试集的划分要尽可能保持数据分布的一致性，避免 因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中 至少要保持样本的类别比例相似.如果从**采样(sampling)的角度来看待数据 集的划分过程，则保留类别比例的采样方式通常称为“分 层 采 样 "(stratified sam p lin g )**

若令训练集S 包含绝大多数样本，则训练出 的模型可能更接近于用D 训练出的模型，但由于T 比较小，评估结果可能不够 稳定准确；若令测试集T 多包含一些样本，则训练集S 与 。 差别更大了，被评 估的模型与用D 训练出的模型相比可能有较大差别，从而降低了评估结果的保 真性(fidelity).这个问题没有完美的解决方案，常见做法是将大约2/3 4/ 5的 样本用于训练，剩余样本用于测试,这个问题没有完美的解决方案，常见做法是将大约2/3 4/ 5的 样本用于训练，剩余样本用于测试.

#### 交叉验证法

![image-20240505004505530](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505004505530.png)

与留出法相似，将数据集。 划 分 为 k 个子集同样存在多种划分方式.为 减小因样本划分不同而引入的差别，k 折交叉验证通常要随机使用不同的划分重复p次,最终的评估结果是这p 次 k 折交叉验证结果的均值，例如常见的有 “10次10折交叉验证”

#### 留一法

假定数据集D 中包含m 个样本，若 令 k = m ，则得到了交叉验证法的一 个特例：留一法(Leave-One-Out,简 称 L O O ).显然，留一法不受随机样本划分，方式的影响，因为m 个样本只有唯一的方式划分为m 个子集— 每个子集包含 一个样本；留一法使用的训练集与初始数据集相比只少了一个样本，这就使得 在绝大多数情况下，留一法中被实际评估的模型与期望评估的用D 训练出的模 型很相似.因此，留一法的评估结果往往被认为比较准确.然而，留一法也有其 缺陷：在数据集比较大时，训练M 个模型的计算开销可能是难以忍受的(例如数 据集包含1 百万个样本，则需训练1 百万个模型)，而这还是在未考虑算法调参 的情况下.另外，留一法的估计结果也未必永远比其他评估方法准确；“没有免 费的午餐”定理对实验评估方法同样适用.

####  自助法

我们希望评估的是用D 训练出的模型.但在留出法和交叉验证法中，由于 保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这 必然会引入一些因训练样本规模不同而导致的估计偏差

“自助法”(bootstrapping)是一个比较好的解决方案，它直接以自助采样 法(bootstrap sampling)为基础[Efron and Tibshirani, 1993]

![image-20240505005018111](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505005018111.png)

这样的测试结果，亦 称 “**包外估计**”(out-of-bag estimate)





### 均方误差

#### 对分布函数p的均方误差

#### 精度与错误率

本章开头提到了错误率和精度，这是分类任务中最常用的两种性能度量, 既适用于二分类任务，也适用于多分类任务.错误率是分类错误的样本数占样 本总数的比例,精度则是分类正确的样本数占样本总数的比例.对样例集。,分![image-20240505011738293](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505011738293.png)![image-20240505011744709](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505011744709.png)类错误率定义为 

更一般的，对于数据分布D和概率密度函p(•),错误率与精度可分别描 述为

![image-20240505011813212](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505011813212.png)







#### P-R图

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505140720019.png" alt="image-20240505140720019" style="zoom: 50%;" />

.然而，在很多情 形下，人们往往仍希望把学习器A 与 B 比出个高低.这时一个比较合理的判据 是比较P -R 曲线下面积的大小，**它在一定程度上表征了学习器在查准率和查全 率上取得相对“双高”的比例**.但这个值不太容易估算，因此，人们设计了一些 综合考虑查准率、查全率的性能度量.

##### 平衡点（Break-Event Point,简 称 BEP）

它 是 “查 准率= 查全率“时的取值，例如图2 .3 中学习器C 的 BEP 是 0.64,而基于BEP 的比较，可认为学习器A 优于B 

##### F1

![image-20240505140959518](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505140959518.png)

![image-20240505141751384](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505141751384.png)

其 中 B > 0 度 量 了 查 全 率 对 查 准 率 的 相 对 重 要 性

![image-20240505141857977](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505141857977.png)



![image-20240505142042103](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240505142042103.png)







### SSIM（结构相似性）











## 线性模型

![image-20240518175244832](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518175244832.png)

线 性 模 型 形 式 简 单 、易于建模,但却蕴涵着机器学习中一些重要的基本思 想 .许 多 功 能 更 为 强 大 的 **非 线 性 模 型** （nonlinear m o d 叫 可 在 线 性 模 型 的 基 础 上 通 过 引 入 层 级 结 构 或 高 维 映 射 而 得 .

.此 外 ，由 于 w直观表达了各属性在预测中 的重要性，因此线性模型有很好的**可解释性（comprehensibility）.**

### 线性回归

- 列向量一般为样本特征

- 横向量为样本个数  ---决定w放置位置
  - 所以y一般横向为样本个数







<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518175513163.png" alt="image-20240518175513163" style="zoom:50%;" />

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518175612565.png" alt="image-20240518175612565" style="zoom:50%;" />

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518175646719.png" alt="image-20240518175646719" style="zoom:50%;" />

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518175926620.png" alt="image-20240518175926620" style="zoom:50%;" />

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518180104952.png" alt="image-20240518180104952" style="zoom:50%;" />

#### 计算欧式距离

基于均方误差最小化来进行模型求解的方法称 为 “最小二乘法”(least square m e t h o d )

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518192600436.png" alt="image-20240518192600436" style="zoom:50%;" />



![image-20240518192722801](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518192722801.png)

### 多元线性回归

![image-20240518192758279](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518192758279.png)

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518194701158.png" alt="image-20240518194701158" style="zoom:50%;" />

![image-20240518231017970](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518231017970.png)

![image-20240518231025782](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518231025782.png)

![image-20240519234558823](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240519234558823.png)

严格凸函数









### 正则化

```py
sss	#一个n*n的矩阵

sss+=np.eye(n)*epsilon
```











![image-20240518231815826](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240518231815826.png)

![image-20240519145136941](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240519145136941.png)

![image-20240519145205742](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240519145205742.png)



### 广义线性模型









![image-20240519195013047](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240519195013047.png)



### 几率

称 为 “几率”(odds),反映了相对可能性。作为正例的相对可能性.对几率取对数则得到 “对数几率”(log odds,亦称logit）





### LDA 线性判别分析

- 考虑协方差时，考虑样本中不同的属性之间的协方差更有用   ---考虑的是不同属性的关系性质


$$
类内协方差均值  = w^TXw
$$
![image-20241104164401398](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241104164401398.png)


$$
\frac{||w^Tμ_0-w^Tμ_1||^2}{w^TX0w+w^TX1w}
$$

- 最大化 Sb/Sw的广义锐利商



#### LDA监督降维技术

![image-20241120205226063](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241120205226063.png)

-  所以为N-1





### 假正率

FP/FP+TF



### 精确率Precision-召回率 Recall



#### 精确率

$$
Precision=\frac{TP}{TP+FP}
$$

#### 召回率

$$
Recall=\frac{TP}{TP+FN}
$$

**回率**是模型在所有真实正类的样本中，成功识别出的比例。它用于衡量模型的“敏感性”，即模型能够找到的所有正类实例的比例。召回率的计算公式为：



### 阈值确定

#### F1优化

$$
F1=2*\frac{Precision*Recall}{Precision+Recall}
$$

- 范围在0-1之间

- 越高越好

```py
from sklearn.metrics import f1_score
current_f1 = f1_score(true_labels,predicted_labels)	#返回f1

```





#### ROC优化

ROC（Receiver Operating Characteristic）曲线展示了不同阈值下的真正率（True Positive Rate, TPR）与假正率（False Positive Rate, FPR）

- 用于找阈值的优化



```py
from sklearn.metrics import roc_curve
fpr,tpr,thresholds =roc_curve(true_labels,predicted_labels)
#返回最佳阈值和对应fpr，tpr
```





## 激活函数

### 饱和型激活函数

#### Sigmoid 函数

$$
σ 
′
 (x)=σ(x)(1−σ(x))
$$



- 当 x 非常大或非常小时，梯度趋近于零（饱和）。
- 容易造成梯度消失问题。

#### tanh函数

$$
\tanh x = \frac{\sinh x}{\cosh x} = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

$$
tanh ^′
 (x)=1−tanh^2
 (x)
$$

- 输出范围是 (−1,1)。
- 类似于 Sigmoid，但中心对称。
- 同样存在梯度消失问题。

#### Softsign 函数

$$
f(x)= \frac{x}{1+∣x∣}
$$

- 输出范围是 (−1,1)(-1, 1)(−1,1)。
- 梯度变化平缓，但仍然会饱和。









### 非饱和型激活函数

#### Soft_max 函数

$$
\text{softmax}(x)_i = \frac{\exp(x_i - \max(x))}{\sum_{j=1}^{K} \exp(x_j - \max(x))}
$$

- 优化输入值非常大或非常小





#### log_softmax  函数







#### Relu



#### 







## 损失函数

- 误差的定义









- 在简单的任务中，损失函数被定义为(yi-wx-b)^2均方误差
  - 没有激活函数 
  - 没有概率
- 可以优化为如下，简化计算

![image-20241026034907604](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241026034907604.png)

- 先选定损失函数，再进行优化，朝着损失函数梯度方向优化

### 二元交叉熵损失函数

$$
L=− \frac{1}{N}​∑^N_{i=1}[y_i​log(p_i​)+(1−y_i​)log(1−p_i​)]
$$

- N 是样本总数。
- yi 是第 i 个样本的真实标签（0 或 1）。
- pi 是模型对第 i 个样本预测为正类的概率，即pi=sigma( )函数


$$
-p_0^{y_0}p_1^{1-y_0}
$$

- p0为0的概率   p1为为1的概率





### 线性估计损失函数




$$
-ln(p_0y_0+p_1(1-y_0))
$$

- 通过线性损失估计函数（最小化）的出来的牛顿法如下：
- <img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241029151821338.png" alt="image-20241029151821338" style="zoom:50%;" />





### BCELoss  二分交叉熵损失

- 误差被定义为**卷积乘积**   
- 范围是0---1

- 需要再调用一个Sigmoid函数先

### BCEWithLogitsLoss






$$
BCEWithLogitsLoss=− \frac{1}{N}∑_{i=1}^{N}[y_iz_i​−log(1+e^{z_i})]
$$

- 直接少了sigmoid函数

### NCE损失函数





### infoNCE损失函数





### 指数函数损失

- 指数函数为单调减函数    

- **误差被定义为乘积**   （如果同号则大   则loss小）     
- 范围是-1---1

$$
\ell_{\exp}(H \mid D) = \mathbb{E}_{x \sim D} \left[ e^{-y_ip_i} \right],
$$

多分类可扩展

 





### MSELoss















## 正则化

- 减少过拟合，提高泛化能力
- 控制复杂性





### 参数正则化  

####   参数范数惩罚

$$
L^′
 (θ)=L(θ)+λR(θ)
$$

##### L1

$$
R(θ)=∥θ∥ _1​= ∑_{i=1}^{n}​∣θ_i∣
$$

##### L2 欧几里得范数

$$
\sqrt[2]{R(θ)}=\sqrt[2]{∥θ∥_2^2}
​ = 
\sqrt[2]{∑_{i}​θ_i^2}
​
$$





##### Elastic Net  结合L1于L2范数惩罚



### 结构正则化  

#### Dropout 正则  （随机丢弃神经元）

假设某个神经元的激活值为 a，并且有概率 p被保留，概率 1−p 被丢弃。

1. **训练阶段**：

   - 如果神经元被保留（概率 p），激活值为 a/p。

   - 如果神经元被丢弃（概率 1−p），激活值为 0。

   - 在训练阶段，神经元的输出期望值为： 

   - $$
     E[输出]=p⋅\frac{a}{p}+(1−p)⋅0=a
     $$

   - 通过除以 p，训练阶段的神经元输出期望值仍然等于 a。

2. **测试阶段**：

   - 测试阶段没有 Dropout，所有神经元都保留，直接使用激活值 a。
   - 因此，测试阶段的神经元输出期望值也是 a。







### 数据驱动正则化 

#### 数据增强



## 归一化



![image-20241205201129939](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241205201129939.png)



### BN  批归一

- 图像

- 不同层的数据分布有差异  所以拉回正态分布后进行计算

$$
μ_B​= \frac{1}{m}∑_{i=1}^{m}​x_i
$$

$$
σ_B^2​= \frac{1}{m}
​∑ 
_{i=1}^m​
(x _i​
 −μ_B​) ^2
$$

$$
x'_{i}​= \frac{x_i-μ_B}{\sqrt[2]{σ_B^2​+ϵ}}
 :
$$

- 最后输出 weight 和bias调整过的参数

$$
y_i​=γ x'_i​+β
$$

- 对同一个batch内的不同样本的经过特征提取后的相同通道（维度）进行以上的计算

  - 假设3*3特征图 ，batch_size =3，16个通道:

  $$
  \begin{gathered}
  \mu_B[c]=\frac{1}{N \cdot H \cdot W} \sum_{n=1}^N \sum_{i=1}^H \sum_{j=1}^W x_{n, c, i, j} \\
  \sigma_B^2[c]=\frac{1}{N \cdot H \cdot W} \sum_{n=1}^N \sum_{i=1}^H \sum_{j=1}^W\left(x_{n, c, i, j}-\mu_B[c]\right)^2
  \end{gathered}
  $$

  

```py
        self.bn1 = nn.BatchNorm2d(out_planes)
        self.relu = nn.ReLU(inplace=True)
```







LN   层归一

- 自然语言







IN   实例归一

- 图像生成







GN   group 归一

- 风格迁移





## 转换函数



### sigmoid函数

- 将其转换为0-1
- 并不互斥







### softmax函数

$$
prob_i= \frac{e^{logit_j}}{∑_{j=1}^{num_{classes}}​e^{logit_j}}
$$

- 转换为概率
- 归一0-1
- 互斥





























### 多分类学习

#### 一对多 (One-vs-Rest, OvR)

- 将原来的多分类问题转换为多个二分类问题，每个分类器用于区分一个特定的类别与其他所有类别。
- 如果有 n 个类别，就需要训练 n 个分类器。
- 最后将每个分类器的输出概率进行比较，选择概率最高的类别作为最终分类结果。

####  一对一 (One-vs-One, OvO)

- 将每对类别之间进行二分类，每两个类别训练一个分类器。
- 如果有 n 个类别，就需要训练 n(n−1)/2 个分类器。
- 在预测时，所有分类器都参与投票，最终选择得票最多的类别作为分类结果。

#### 分层预测

- 将类别抽象为一个集合，然后二分类是否属于这个集合

#### 多对多(MVM) 

- **Many-vs-Many (MvM)** 分类器是一种将多个类别一起分为两组，然后构造多个分类器，每个分类器的任务是区分一组类别与另一组类别的区别。

- 这种方法不是简单地把每个类别分开，而是把多个类别组合在一起，构建不同的二分类器，训练它们分别对多个类别的组合进行分类。









## 总结

- 监督学习
  - 分类
    - 在二分类中，（logistic回归）会根据0.5进行判断，映射为二分类
    - logistic回归 ： 实数=====>0----1
  - 回归
- 损失函数用于迭代优化参数值
- 代数回归计算计算出初始值

















## 决策树







### 划分

由算法4.2可看出，决策树学习的关键是第8 行，即如何选择最优划分属 性.一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“**纯度**”(purity)越来越高.

#### **信息熵**：

$$
H(D)=−∑^n_{i=1}p_ilog(p_i)
$$
- 可以推断出，是在0，1的一个凸包，在两侧的值会小，中间的值会大





#### **信息增益**：

$$
信息增益=H(D)−∑_v\frac{∣Dv∣}{∣D∣}H(D_v)
$$
其中 Dv 是按某个特征划分后的数据子集。

- 越大越好

- min_gain_split 为决定H(d)的最小值

- max_depth 为树的深度
- min_sample_split 为一个分化内的最小样本数







```py
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor


#构建决策树





#构建回归树

```



### 过程

#### 停止

```py
if depth >= self.max_depth or n_labels ==1 or n_samples<self.min_sample_split:
            #计算最常见的label
            leaf_value=self._most_common_label(y)
            return Node(value=leaf_value)
```

- 到达最大深度，或者父节点个数小于min_split
- 返回最一般的label



```py
def gene_tree(self,X,y,depth=0)
	split_thres,split_idx: #选择最佳分割点，决定阈值和特性idx

```



### 减枝

 **预剪枝**

- 设置min_sample_split（限制数量）
- 设置max_depth（限制深度）

- 设置min_threshold（限制阈值过纯）

**后剪枝 (Post-pruning)**

- 递归减枝左右子树

```py
# 递归剪枝左右子树
        left_tree = node["left"]
        right_tree = node["right"]
        self._prune_tree(left_tree, X_val[left_idxs], y_val[left_idxs])
        self._prune_tree(right_tree, X_val[right_idxs], y_val[right_idxs])

        
#减枝条件

#now-predict

node_value =self._most_common_label(y_val)
node["tpye"]="leaf"
node["value"]=node_value
del node["left"]
del node["right"]


#afert-predict

#如果下降
#回复
           if acc_prune < acc_no_prune:
                node["type"] = "node"
                node["left"] = left_tree
                node["right"] = right_tree
                del node["value"]

```



### 连续值

#### 二分法



### 缺失值

- ρ的引入
  - **权重调整**：由于缺失值的存在，无缺失值样本的数量通常少于总样本数量。直接使用无缺失值样本的信息增益可能会高估其重要性。通过乘以 ρ*ρ*，可以将信息增益调整为相对于总样本的比例，使其更具代表性。

- 计算考虑无缺失值/总值   作为占比











## CART决策树



#### 基尼系数

$$
Gini(D)=1−∑_{k=1}^{K}p^2_k
$$



- 基尼指数越低，数据纯度越高。






## 多变量决策树

- 选择一个特征的线性组合和一个阈值t去比较

$$
w^TX+b<=t
$$









## 分类



### 多分类

#### 多标签分类

- 对于 n 个类别，模型输出一个大小为 [batch_size,n] 的张量，通过 `sigmoid` 激活函数将每个类别的得分独立转换为概率：



#### 单标签分类

- 对于 n 个类别，模型输出一个大小为[batch_size,n] 的张量，通过 `softmax` 激活函数将其转换为概率分布。









## 优化器

### SGD

SGD带动量的参数更新公式如下：
$$
v_t​ =γv_{t−1}+η∇L(θ_t)
$$

$$
𝜃_{𝑡+1}=𝜃_𝑡−𝑣_𝑡  
$$

- γ：动量因子（Momentum coefficient），通常取值在[0.8, 0.99]。

- η：学习率。
- θt：当前参数值。











### Adam

- Adaptive Moment Estimation

- 自适应学习率优化





### 学习器调度器 lr_scheduler















## 反向传播

### 牛顿迭代法   --基本用一阶



### 梯度下降法  高阶多阶矩阵





#### 全量梯度下降

- （Full Batch Gradient Descent）

#### 批量梯度下降

- （Batch Gradient Descent, SGD）













## 神经网络

### 激活函数

####  **ReLU（Rectified Linear Unit）**   

$$
f(x)=max(0,x)
$$

- ReLU 的图像看起来像是将负数部分直接截为 **0**，而正数部分保持原样。

- 这意味着 **ReLU** 具有非线性特性，并且可以有效地增加模型的表达能力。

**Leaky ReLU**：给负输入一个很小的斜率，而不是直接置为零，例如：0.01x

**Parametric ReLU (PReLU)**：类似于 **Leaky ReLU**，但负数部分的斜率是一个可学习的参数。

**ELU (Exponential Linear Unit)**：负数部分采用指数衰减形式，以帮助应对“Dying ReLU”问题。





#### SIGMOID函数

#### 阶跃函数









### 感知机与多层网络

**感知机**(Perceptron)由两层神经元组成，如 图 5 .3 所示，输入层接收外界输入信号后传递给输出层，输 出 层 是 M-P 神经元，亦 称 “**阈值逻辑单元**”(threshold logic unit).







更一般地，给 定 训 练 数 据 集 ,权 重 wi(i= 1,2,...,n ) 以 及 阈 值 theta可通过学 习 得 到 .阈 值 theta可 看 作 一 个 固 定 输 入 为 —1.0 的 “哑 结 点 ”( d u m m y n o d e )所对 应 的 连 接 权 重 w n + 1这 样 ，权 重 和 阈 值 的 学 习 就 可 统 一 为 权 重 的 学 习 .感 知 机 学 习 规 则 非 常 简 单 ，对 训 练 样 例 (x,y) ，若 当 前 感 知 机 的 输 出 为 y^,则感知机权 重将这样调整：

![image-20240517015800214](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240517015800214.png)







感 知 机 只 有 输 出 层 神 经 元 进 行 激 活 函 数 处 理 ，即只拥有一层 **功能神经元** (functional neuron)

事 实 上 ，上 述 与 、或 、 非问题都是线性可分(linearly separable)的 问 题 .可 以 证 明 [Minsky and Papert, 1969],若 两 类 模 式 是 线 性 可 分 的 ，即 存 在 一 个 线 性 超 平 面 能 将 它 们 分 开 ，如图 5.4(a)-(c)所 示 ，则感知机的学习过程一定会**收敛**(converge)而求得适当的权向 量 w = (wi；W 2 ；.. .;w n + i);否则感知机学习过程将会发生振荡(Huctuation), w 难 以 稳 定 下 来 ，不 能 求 得 合 适 解 ，例 如 感 知 机 甚 至 不 能 解 决 如 图 5.4(d)所示的 异或这样简单的非线性可分问题

### 多层前馈神经网络

更一般的，常见的神经网络是形如图5 .6所示的层级结构，每层神经元与下 一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接.这样的 神经网络结构通常称为“多层前馈神经网络" (multi-layer feedforward neural networks),其中**输入层神经元**接收外界输入，**隐层与输出层**神经元对信号进行加工，最终结果由输出层神经元输出；换言之，输入层神经元仅是接受输入，不进行函数处理，隐层与输出层包含功能神经元.因此，图 5.6(a)通常被称为“两层网络”.为避免歧义，本书称其为“单隐层网络”.只需包含隐层，即可称 为多层网络.**神经网络的学习过程，就是根据训练数据来调整神经元之间的** “**连接权**”(connection weight)以及每个功能神经元的**阈值**；换言之，神经网 络 “学”到的东西，蕴涵在连接权与阈值中，

![image-20240517020123097](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240517020123097.png)



#### 前馈

![image-20240517020300602](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20240517020300602.png)

### 误差逆传播算法	BP算法

- 反向传播
- 实际上这个b在感知机中被替换为了阈值，这个阈值在最初是手动输入
- 后面被优化为任意值

$$
z=f(wx+b)
$$

- 输出层的激活权重v

$$
y'​=vz
$$

- 通过损失函数更新，求v，w的偏导

### 积累误差逆传播算法---批量梯度下降

- 读取完所有的后进行逆传播















### 误差逆传播

- 隐藏状态权重矩阵 Wh  表示从前一个时间步的隐藏状态到当前隐藏状态的连接权重

- 输入权重矩阵 Wx 表示从输入到隐藏状态的连接权重。

- 隐藏状态向量 ht 可以看作是网络在时间步 t 对输入信息的“记忆”或“总结”







## 支持向量机

![image-20241225165810330](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225165810330.png)



- 软间隔和硬间隔

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225170401452.png" alt="image-20241225170401452" style="zoom: 33%;" />













- 点到超平面的距离
- w是法向量        半径距离 （原点到平面w^T x + b =c的距离）

$$
r = \frac{|w^T x + b|}{\|w\|}
$$

- 异类点间隔 
  
  - 对wt+b的多余限制 
  
  $$
  \gamma = \frac{2}{\|w\|}
  $$
  <img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241205205102460.png" alt="image-20241205205102460" style="zoom: 67%;" />
  
- $$
  y_i \left( \vec{w} \cdot \vec{x}_i + b \right) \geq 1
  $$

- 即找到w和b，使得γ最大：

$$
\begin{aligned}
& \underset{\mathbf{w}, b}{\text{max}} & & \frac{2}{\|\mathbf{w}\|} \\
& \text{s.t.} & & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \ldots, m.
\end{aligned}
$$

- 也可以写作||w||的倒数

$$
\begin{aligned}
& \underset{\mathbf{w}, b}{\text{min}} & & \frac{1}{2} \|\mathbf{w}\|^2 \\
& \text{s.t.} & & y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \ldots, m.
\end{aligned}
$$

- 松弛变量

$$
L(w, b, \lambda_i, p_i) = \frac{\|\vec{w}\|^2}{2} - \sum_{i=1}^{s} \lambda_i \left( y_i (\vec{w} \cdot \vec{x}_i + b) - 1 - p_i^2 \right), \quad i = 1, 2, 3, \ldots, s
$$

- 惩罚系数λi

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225172643988.png" alt="image-20241225172643988" style="zoom:67%;" />







### kkt条件

![image-20241225173147750](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225173147750.png)

- 可以看作以下约束求解
- 求解最小化

$$
\text{minimize} \quad f(w) = \frac{\|\vec{w}\|^2}{2}, \quad \|\vec{w}\| = \sqrt{w_1^2 + w_2^2}
$$

$$
\text{subject to} \quad g_i(w, b) = y_i \left( \vec{w} \cdot \vec{x}_i + b \right) - 1 \geq 0, \quad i = 1, 2, 3, \ldots, s
$$

-  转换为xi固定后，求解w

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225174615811.png" alt="image-20241225174615811" style="zoom:33%;" />

- 调整梯度

![image-20241225174732606](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225174732606.png)

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225174904740.png" alt="image-20241225174904740" style="zoom:50%;" />

- kkt条件  4个偏导=0 和一个>=0

$$
\vec{w} - \sum_{i=1}^{s} \lambda_i y_i \vec{x}_i = 0
$$

$$
-\sum_{i=1}^{s} \lambda_i y_i = 0
$$

$$
y_i \left( \vec{w} \cdot \vec{x}_i + b \right) - 1 \geq 0
$$

$$
\lambda_i \left( y_i \left( \vec{w} \cdot \vec{x}_i + b \right) - 1 \right) = 0
$$

$$
\lambda_i \geq 0
$$





### 升维转换 与 kernel trick





#### 对偶问题

- 直接求原问题的最小值

- 求约束对目标函数的最优影响

- 

- $$
  \min_{x} f(x) \quad \text{等价于} \quad \min_{x} \max_{\lambda \geq 0} L(x, \lambda)
  $$

  $$
  \max_{\lambda \geq 0} \min_{x} L(x, \lambda)
  $$



- 求解最大化

- q(λi)：构造

$$
q(\lambda_i) = \underset{\lambda_i}{\text{minimize}} \left( L(w, b, \lambda_i) \right) = \underset{\lambda_i}{\text{minimize}} \left( f(w) - \sum_{i=1}^{s} \lambda_i \cdot g_i(w, b) \right) \leq f(\vec{w}^*) - \sum_{i=1}^{s} \lambda_i \cdot g_i(\vec{w}^*, b^*)
$$


$$
q(\lambda_i) \leq f(\vec{w}^*) \leq f(w)
$$

$$
q(\lambda_i) \leq q(\lambda_i^*) \leq f(\vec{w}^*) \leq f(w)
$$

- 即转换为求解

$$
\text{maximize} \quad q(\lambda_i) = \underset{\lambda_i}{\text{maximize}} \left( \underset{(w, b)}{\text{minimize}} \left( L(w, b, \lambda_i) \right) \right)
$$

$$
\text{subject to} \quad \lambda_i \geq 0, \quad i = 1, 2, 3, \ldots, s
$$

![image-20241225180830950](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225180830950.png)




$$
\text{maximize} \quad q(\lambda_i) = \underset{\lambda_i}{\text{maximize}} \left( \sum_{i=1}^{s} \lambda_i - \frac{1}{2} \sum_{i=1}^{s} \sum_{j=1}^{s} \lambda_i \lambda_j y_i y_j \vec{x}_i \cdot \vec{x}_j \right)
$$

$$
\text{subject to} \quad \lambda_i \geq 0, \quad i = 1, 2, 3, \ldots, s
$$

根据 
$$
\lambda_i (y_i (\vec{w} \cdot \vec{x}_i + b) - 1) = 0
$$
 仅支持向量参与计算,其余λ都=0

![image-20241225183112779](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225183112779.png)

#### 核技巧

$$
 T(\vec{x}_i) \cdot T(\vec{x}_j) = K(\vec{x}_i, \vec{x}_j) 
$$


$$
K(\vec{x}_i, \vec{x}_j) = (c + \vec{x}_i \cdot \vec{x}_j)^d 
$$

##### RBF核函数   拉普拉斯核函数

$$
K(\vec{x}_i, \vec{x}_j) = e^{-\gamma \|\vec{x}_i - \vec{x}_j\|^2}
$$



- 核函数组合

$$
\gamma_1 \kappa_1 + \gamma_2 \kappa_2
$$

$$
\kappa_1 \otimes \kappa_2(\vec{x}, z)= \kappa_1(\vec{x}, z) \kappa_2(\vec{x}, z)
$$

$$
\kappa(\vec{x}, z) = g(\vec{x}) \kappa_1(\vec{x}, z) g(z)
$$

### 软间隔

- hinge loss function

![image-20241225190438542](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225190438542.png)


$$
\text{minimize} \quad f(\vec{w}) = \frac{\|\vec{w}\|^2}{2} + C\sum_{i=1}^{s} \varepsilon_i
$$
其中

- 增加C的容忍度

$$
\varepsilon_i = \max(0, 1 - y_i (\vec{w} \cdot \vec{x}_i + b))
$$

- 增加约束
- ![image-20241225190913292](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241225190913292.png)









## 贝叶斯分类器

x分类为ci的**条件风险**

- **入ij**为将**j**误判为**i**的概率



- 将x分为ci的条件风险

$$
R(c_i \mid \boldsymbol{x}) = \sum_{j=1}^{N} \lambda_{ij} P(c_j \mid \boldsymbol{x}).
$$



- 对于条件风险   为1-本来分为c正确

$$
R(c \mid \boldsymbol{x}) = 1 - P(c \mid \boldsymbol{x}),
$$


$$
h^*(\boldsymbol{x}) = \arg \max_{c \in \mathcal{Y}} P(c \mid \boldsymbol{x}),
$$


- 最小化风险   





### 贝叶斯判定准则

- 贝叶斯风险 R（x）

$$
h(x) = \arg \min_{c_i} R(c_i \mid x),
$$













$$
P(c \mid \boldsymbol{x}) = \frac{P(c) P(\boldsymbol{x} \mid c)}{P(\boldsymbol{x})},
$$

-   P(x)是 用于归一化的“**证据**"(evidence)因子

- P(x)为归一化因子， 而主要是估计上面





### P(C)

- 使用频率去估计概率





### likehood

- 对likehood进行估计
- 例如，假设样本的d个属性都是二值的，则样本空间将有2^d种可能的取值，在现实应用中，这个值往往 远大于训练样本数m



- 假设对于类型的决定由参数决定

- 所以用似然去估计决定类型的参数 
- 采用样本集去似然

$$
P(D_c \mid \theta_c) = \prod_{\pmb{x} \in D_c} P(\pmb{x} \mid \theta_c) \,.
$$

- 一般采用对数似然

$$
\hat{\theta}_c = \arg\max_{\theta_c} LL(\theta_c).
$$













### 朴素贝叶斯分类器

- 考虑对所有属性 c1，c2，...他们分布相互独立

- 属性条件独立性假设
- xi为x在i属性的取值

$$
P(c \mid \pmb{x}) = \frac{P(c) P(\pmb{x} \mid c)}{P(\pmb{x})} = \frac{P(c)}{P(\pmb{x})} \prod_{i=1}^{d} P(x_i \mid c) \,,
$$










$$
h_{nb}(\boldsymbol{x}) = \arg\max_{c \in \mathcal{Y}} P(c) \prod_{i=1}^{d} P(x_i \mid c),
$$

- 求解其损失 -----考虑对x的对数概率预测
-  关于连续分布

$$
\log P(x \mid c) = -\frac{1}{2} \sum_{i=1}^{d} \left( \frac{(x_i - \mu_{c,i})^2}{\sigma_{c,i}^2} + \log(\sigma_{c,i}^2) \right)
$$











#### 拉普拉斯修正器

- 对于离散的分布 MLE会直接用样本频率来估计概率，并不会直接求解他的分布参数 
- 可能会出现0
- 需要修正

$$
\hat{P}(c) = \frac{|D_c| + 1}{|D| + N} \,,

\hat{P}(x_i \mid c) = \frac{|D_{c, x_i}| + 1}{|D_c| + N_i} \,.
$$







### 半朴素贝叶斯分类器



`







## 集成学习

- 同质  异质
- 基学习器  base learner  组件学习器





### boosting

#### 加性模型

- 调整分布
- 迭代计算
- loss exp-(AtHtx)fx最小
  - at反向传播
  - htx反向传播







- 误差定义
- 第i个分类器的预估函数 与实际函数不相等的概率

$$
P(h_i(\boldsymbol{x}) \neq f(\boldsymbol{x})) = \epsilon.
$$







- 由于对数据有权重  所以是加权误差


$$
\epsilon_i = \sum_{x \in D} P(x) \cdot \mathbb{1}(h_i(x) \neq f(x)),
$$

- h(x) 为所定义的分类器  （比如各种基函数）





- 最终加函数
  - α为权重

$$
H(\boldsymbol{x}) = \sum_{t=1}^{T} \alpha_t h_t(\boldsymbol{x})
$$

![image-20241212053536234](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241212053536234.png)



- α ： 模型的强弱   弱分类器的权重

- Zt：归一化因子

- 分类错误后，会被**提高权重**

##### 替代函数的证明

- **如果加函数可使损失最小化**       **则分类错误也最小化**  **所以是替代函数**

- 对H (X)进行求导    =0

$$
H(\boldsymbol{x}) = \frac{1}{2} \ln \frac{P(f(\boldsymbol{x}) = 1 \mid \boldsymbol{x})}{P(f(\boldsymbol{x}) = -1 \mid \boldsymbol{x})},
$$

- 对H(X)   求sign   过半则判定为1

$$
\text{sign}(H(\boldsymbol{x})) = \text{sign}\left(\frac{1}{2} \ln \frac{P(f(\boldsymbol{x}) = 1 \mid \boldsymbol{x})}{P(f(\boldsymbol{x}) = -1 \mid \boldsymbol{x})}\right)
$$

$$
= 
\begin{cases} 
1, & \text{if } P(f(\boldsymbol{x}) = 1 \mid \boldsymbol{x}) > P(f(\boldsymbol{x}) = -1 \mid \boldsymbol{x}) \\
-1, & \text{if } P(f(\boldsymbol{x}) = 1 \mid \boldsymbol{x}) < P(f(\boldsymbol{x}) = -1 \mid \boldsymbol{x})
\end{cases}
$$

$$
= \arg\max_{y \in \{-1, 1\}} P(f(\boldsymbol{x}) = y \mid \boldsymbol{x})
$$

















##### 关于更新的αt*h(x)的推导 

- α的更新


$$
\begin{equation}
\begin{aligned}
\ell_{\exp}(\alpha_t h_t \mid D_t) &= \mathbb{E}_{\boldsymbol{x} \sim D_t} \left[ e^{-f(\boldsymbol{x}) \alpha_t h_t(\boldsymbol{x})} \right] \\
&= \mathbb{E}_{\boldsymbol{x} \sim D_t} \left[ e^{-\alpha_t} \mathbb{I}(f(\boldsymbol{x}) = h_t(\boldsymbol{x})) + e^{\alpha_t} \mathbb{I}(f(\boldsymbol{x}) \neq h_t(\boldsymbol{x})) \right] \\
&= e^{-\alpha_t} P_{\boldsymbol{x} \sim D_t}(f(\boldsymbol{x}) = h_t(\boldsymbol{x})) + e^{\alpha_t} P_{\boldsymbol{x} \sim D_t}(f(\boldsymbol{x}) \neq h_t(\boldsymbol{x})) \\
&= e^{-\alpha_t} (1 - \epsilon_t) + e^{\alpha_t} \epsilon_t,
\end{aligned}
\tag{8.9}
\end{equation}
$$


- 对于更新到第t次分布



##### 目的

- 减少h(x) -->基于Dt(X)进行训练
- h(x)将在新的分布下进行训练 






- AdaBoost算法在获得之后样本分布将进行调整，使下一轮的基学习 器ht 能纠正Ht-1 的一些错误.理想的ht能纠正区一的全部错误，即最小化

$$
\ell_{\exp}(H_{t-1} + h_t \mid D) = \mathbb{E}_{\boldsymbol{x} \sim D} \left[ e^{-f(\boldsymbol{x})(H_{t-1}(\boldsymbol{x}) + h_t(\boldsymbol{x}))} \right]
$$

- 最小化推导出   **ht 是基于dt轮次** 的lossfunc
  
- $$
  h_t(\boldsymbol{x}) = \arg\min_{h} \ell_{\exp}(H_{t-1} + h \mid \mathcal{D})
  $$
  
  $$
  h_t(\boldsymbol{x}) = \arg\min_{h} \mathbb{E}_{\boldsymbol{x} \sim D_t} \left[ \mathbb{I}(f(\boldsymbol{x}) \neq h(\boldsymbol{x})) \right].
  $$
  









**更新**

- 分布之间的关系
- Dt+1 的分布由at（ht决定）

- DT为一个分布

$$
D_t(\boldsymbol{x}) = \frac{D(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim D}[e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}]}.
$$

$$
\begin{equation}
\begin{aligned}
\mathcal{D}_{t+1}(\boldsymbol{x}) &= \frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_t(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) H_t(\boldsymbol{x})} \right]} \\
&= \frac{\mathcal{D}(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})} e^{-f(\boldsymbol{x}) \alpha_t h_t(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) H_t(\boldsymbol{x})} \right]} \\
&= \mathcal{D}_t(\boldsymbol{x}) \cdot e^{-f(\boldsymbol{x}) \alpha_t h_t(\boldsymbol{x})} \frac{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})} \right]}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}} \left[ e^{-f(\boldsymbol{x}) H_t(\boldsymbol{x})} \right]},
\end{aligned}
\tag{8.19}
\end{equation}
$$













- 有以下分布的归一化可行   

$$
\mathcal{D}_t(\boldsymbol{x}) = \frac{D(\boldsymbol{x}) e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}}{\mathbb{E}_{\boldsymbol{x} \sim \mathcal{D}}[e^{-f(\boldsymbol{x}) H_{t-1}(\boldsymbol{x})}]} \, ,
$$





![image-20241212180106913](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241212180106913.png)



**归一化因子**   在最后一行



































#### 例子

- 权重影响损失函数    --交叉熵损失函数   

$$
\mathcal{L} = - \sum_{i=1}^{m} D_i(\boldsymbol{x}_i) \cdot \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right],
$$













### 随机森林

#### Bagging





- 对于33.6%的外包预测

$$
H^{oob}(\pmb{x}) = \arg\max_{y \in \mathcal{Y}} \sum_{t=1}^{T} \mathbb{I}(h_t(\pmb{x}) = y) \cdot \mathbb{I}(\pmb{x} \notin D_t) \,,
$$

- 相关的外包预测的**泛化误差**
  - 指示函数  **外包预测不正确的总数** / **外包样本总数**

$$
\epsilon^{oob} = \frac{1}{|D|} \sum_{(\pmb{x}, y) \in D} \mathbb{I}(H^{oob}(\pmb{x}) \neq y) \,.
$$



#### 结合方法

- 平均法

$$
H(\pmb{x}) = \frac{1}{T} \sum_{i=1}^{T} h_i(\pmb{x}) \,.
$$

$$
H(\pmb{x}) = \sum_{i=1}^{T} w_i h_i(\pmb{x}) \,.
$$

- 投票法

  - **硬投票**和**软投票**

  - 对n个维度的j维度进行求和   判断是否超过所有n维的一半
  - h  为分类器（1-T）   k为维度

  $$
  H(\pmb{x}) = 
  \begin{cases} 
  c_j, & \text{if } \sum_{i=1}^{T} h_i^j(\pmb{x}) > 0.5 \sum_{k=1}^{N} \sum_{i=1}^{T} h_i^k(\pmb{x}) ; \\
  \text{reject}, & \text{otherwise}.
  \end{cases}
  $$

  - **相对多数投票法**

$$
H(\pmb{x}) = c_{\arg\max_j \sum_{i=1}^{T} w_i h_i^j(\pmb{x})} \,.
$$

- 学习法

- stacking学习法    基学习器   次级学习器

<img src="C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241214171821479.png" alt="image-20241214171821479" style="zoom: 67%;" />











## 聚类

### 聚类性能度量

- 聚类“有效性指标”(validity index)
-  聚类结果的“簇内相似 度”(intra-cluster similarity)高 且 “簇间相似度”(inter-cluster similarity)低
- 定义 D 为**样本集**   C为**聚类划分**                  样本集   C*为**标准分类**

$$
D = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m\}
$$

$$
 C = \{C_1, C_2, \ldots, C_k\}
$$

$$
C^* = \{C_1^*, C_2^*, \ldots, C_s^*\}
$$







$$
a &= |SS|, \quad SS = \{ (\mathbf{x}_i, \mathbf{x}_j) \mid \lambda_i = \lambda_j, \lambda_i^* = \lambda_j^*, i < j \}, \tag{9.1} \\
$$

$$
d &= |DD|, \quad DD = \{ (\mathbf{x}_i, \mathbf{x}_j) \mid \lambda_i \neq \lambda_j, \lambda_i^* \neq \lambda_j^*, i < j \}.\tag{9.4} \\
$$



- 错 - 本应不同

$$
b &= |SD|, \quad SD = \{ (\mathbf{x}_i, \mathbf{x}_j) \mid \lambda_i = \lambda_j, \lambda_i^* \neq \lambda_j^*, i < j \}, \tag{9.2} \\
$$

- 错 -本应同

$$
c &= |DS|, \quad DS = \{ (\mathbf{x}_i, \mathbf{x}_j) \mid \lambda_i \neq \lambda_j, \lambda_i^* = \lambda_j^*, i < j \}, \tag{9.3} \\
$$



- JC        t / t +f +f     up

$$
\text{JC} = \frac{a}{a + b + c}.
$$

- Rand     t+t/ sum   up

$$
    \text{RI} = \frac{2(a + d)}{m(m - 1)}.
$$

- FMI    t/t+f1 t/t+f2    up

$$
    \text{FMI} = \sqrt{\frac{a}{a + b} \cdot \frac{a}{a + c}}.
$$





### 簇距离

#### 簇内

- avg(C) 簇内平均距离

$$
\text{avg}(C) = \frac{2}{|C|(|C| - 1)} \sum_{1 \leq i < j \leq |C|} \text{dist}(\mathbf{x}_i, \mathbf{x}_j),
$$

- C为类C的样本数





- diam(C)

$$
\text{diam}(C) = \max_{1 \leq i < j \leq |C|} \text{dist}(\mathbf{x}_i, \mathbf{x}_j),
$$

- 簇直径 





#### 簇间

- 对簇分类后  为簇间距离

$$
\text{dist}(C_1, C_2) = \frac{\sum_{x \in C_1, y \in C_2} \text{dist}(x, y)}{|C_1| \cdot |C_2|}
$$







- dmin(C1,C2)


$$
d_{\min}(C_i, C_j) = \min_{\mathbf{x}_i \in C_i, \mathbf{x}_j \in C_j} \text{dist}(\mathbf{x}_i, \mathbf{x}_j),
$$

- dcen(C1,C2)  中心距离

$$
d_{\text{cen}}(C_i, C_j) = \text{dist}(\mu_i, \mu_j),
$$

### 簇分类指数

#### DMI 指数

- 针对每一个簇，1/k*max( Σ 类内avg c  / 类间中心间距)
- 越小越好

$$
\text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \left( \frac{\text{avg}(C_i) + \text{avg}(C_j)}{d_{\text{cen}}(\mu_i, \mu_j)} \right).
\tag{9.12}
$$

#### Dunn指数

- min   **所有**类间min距离  / max **所有**类内直径    
- 越大越好

$$
\text{DI} = \min_{1 \leq i \leq k} \left\{ \min_{j \neq i} \left( \frac{d_{\min}(C_i, C_j)}{\max_{1 \leq l \leq k} \text{diam}(C_l)} \right) \right\}.
\tag{9.13}
$$

### 距离

- 闵可夫斯基距离
- 可以带有加权

$$
\text{dist}_{\text{mk}}(\boldsymbol{x}_i, \boldsymbol{x}_j) = \left( \sum_{u=1}^{n} |x_{iu} - x_{ju}|^p \right)^{\frac{1}{p}}
$$

- 无序属性

  -  VDM 

  - $$
    \text{VDM}_p(a, b) = \sum_{i=1}^{k} \left| \frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}} \right|^p
    $$

- 有序属性

#### 距离度量学习

### 原型聚类   

### k-聚类  算法    无标注

$$
E = \sum_{i=1}^{k} \sum_{\boldsymbol{x} \in C_i} \| \boldsymbol{x} - \boldsymbol{\mu}_i \|_2^2
$$

更新中心

- 中心更新直接分类完后加权





### 学习向量量化  LVQ   算法   有标注

更新原型向量

- 寻找最近的原型向量

- prototype=prototype+*η*⋅(sample−prototype)(类别相同)
- prototype=prototype−η⋅(sample−prototype)(类别不同)



- 向量量化  Voronoi 划分



### 高斯混合聚类   数学

#### EM算法

- 计算每个样本的对类k的贡献度

- 为样本i的关于类k的后验概率  

$$
\gamma_{ik} = \frac{\pi_k \cdot \mathcal{N}(x_i \mid \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \cdot \mathcal{N}(x_i \mid \mu_j, \Sigma_j)}
$$

- 更新 pi
- 考虑最大似然  
- 同时满足和为1 
- 考虑LL(D)   +λ(Σai-1) 拉格朗日 

$$
\pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
$$

![image-20241228183426458](C:\Users\Tayhirro\AppData\Roaming\Typora\typora-user-images\image-20241228183426458.png)













- 更新均值   更新 方差
- 考虑最大似然

$$
\mu_k = \frac{\sum_{i=1}^{N} \gamma_{ik} x_i}{\sum_{i=1}^{N} \gamma_{ik}}
$$

$$
\Sigma_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (x_i - \mu_k)(x_i - \mu_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}
$$



### 密度聚类   算法

####  DBSCAN   

1. **核心点**：在 eps 半径内有至少 minPts 个点的点。
2. **边界点**：在 eps 半径内的点数量少于 minPts，但属于某个核心点的邻域。
3. **噪声点**：既不是核心点，也不是边界点的点。



- BFS遍历所有**核心点**





- 核心点对象集合  A   
- 聚类数  k
- 未访问样本集合 T 
  - 取出T --->q集合
  - 找出半径类密度可达点  



### 层次聚类   算法

- 凝聚层次聚类

- 归并算法   AGNES  

- 单连接  全连接  均连接







- 分裂层次聚类

- DIANA 算法



- 找到dist far 的点 二分裂 

- 选择加入

  







## 强化学习

T**步累积奖赏 (n-step return)**:
$$
\mathbb{E} \left[ \frac{1}{T} \sum_{t=1}^{T} R_t \right]
$$

**扣累积奖赏 (discounted return)**:
$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$




















# 计算图

## 静态图与动态图

- 在调用的时候才进行**动态的图**的传播路径构造
- **静态图**是在最初的时候就初始化整个图的传播路径



## tensor的反向传播

- tensor内部有一个存放梯度的区域 ，会和神经网络自动连接

```
criterion = nn.BCELoss() #定义一个损失函数
optimizer = optim.Adam(model.parameters,lr = learning_rate)
```

- 当tensor执行反向传播的时候，所有梯度已经自动更新，但是还没有赋值

```
loss = criterion(outputs,)
```

















# CUDA

#### **CUDA 的基础并行单元**

- **线程（Thread）**：最基本的并行单元，负责执行代码。
- **线程块（Thread Block）**：**一组线程**，具有**共享内存**，运行在同一个**流多处理器**（SM）上。
- **网格（Grid）**：由**多个线程块**组成，网格内的线程可以访问**全局内存**。

#### **GPU 的计算层级**

- **流多处理器（Streaming Multiprocessor, SM）**：GPU 的核心模块，每个 SM 包含多个 CUDA 核心，可以同时执行多个线程块。
- **CUDA 核心（CUDA Core）**：执行单个线程的硬件单元。



- batch_size于thread一个线程块相乘，获得总线程数









#  数据集







## coco













- info

- licenses

- images
  - id
  - name
- **annotations**
  -  **categry_id**
  - **image_id**
  - name
  
- categories
  - cat-id

### coco.dataset   

- 包含coco的所有键和值

```py
coco.dataset.keys()
coco.dataset['']	#具体键值对
```



### COCO

```
from pytools.coco import COCO

coco=COCO(coco_json)
```

#### ge(Cat)Ids

-  获取id

- 返回一个id数组

- ```
  categories, catNms=None, supNms=None, catIds=None
  ```

#### load(Cat)s

- 通过id数组进行json数组加载

- 返回一个json数组







### annotations

- segmentations   精细化的轮廓框
- bbox   边界框







# 马尔可夫链

## 转移概率矩阵

经过一定有限次数序列的转换，最终一定可以得到一个**稳定的概率分布** ，且与初始状态概率分布无关。



- 初始概率    每一个**词性标签** 放在开头的概率  

```
dict =[("value","词性标签"),]
```



- **状态转移概率**   **词性标签**转移到下一个**词性标签**的概率



- **发射概率**   给定词性标签的情况下，某一个词语出现的概率

- $$
  P(word|State)
  $$

- **时间步**    数据的顺序位置



























#   交叉验证

- k折交叉验证